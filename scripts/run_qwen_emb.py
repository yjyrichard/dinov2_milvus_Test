import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image
import requests
from io import BytesIO

# ================= é…ç½®å‚æ•° =================
MODEL_PATH = "Qwen/Qwen3-VL-Embedding-2B"  # æ¨¡å‹IDï¼Œç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½(~4GB)
OUTPUT_DIM = 768  # ä½ æƒ³è¦çš„ç»´åº¦ï¼å¯ä»¥æ”¹æˆ 512, 1024 ç­‰
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {DEVICE}")
if DEVICE == "cuda":
    print(f"   æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")

# ================= 1. åŠ è½½æ¨¡å‹ =================
print("1. æ­£åœ¨åŠ è½½æ¨¡å‹ (ç¬¬ä¸€æ¬¡ä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")

try:
    # âš ï¸ å…³é”®ç‚¹ï¼š970M å¿…é¡»ç”¨ float16ï¼Œä¸èƒ½ç”¨ bfloat16
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.float16,  # å¼ºåˆ¶ä½¿ç”¨ FP16
    ).to(DEVICE)
    
    processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–æ˜¾å­˜ä¸è¶³ã€‚\né”™è¯¯ä¿¡æ¯: {e}")
    exit()

# ================= 2. å‡†å¤‡æµ‹è¯•æ•°æ® =================
# ä¸‹è½½ä¸€å¼ æµ‹è¯•å›¾ç‰‡
img_url = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
try:
    image = Image.open(BytesIO(requests.get(img_url).content))
except:
    # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç©ºç™½å›¾ä»£æ›¿
    image = Image.new('RGB', (224, 224), color='white')
    print("âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Œä½¿ç”¨ç©ºç™½å›¾æµ‹è¯•")

# å®šä¹‰è¾“å…¥ï¼šä¸€æ®µæ–‡æœ¬ + ä¸€å¼ å›¾ç‰‡
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": "Describe this image"},
        ],
    }
]

# ================= 3. æ•°æ®é¢„å¤„ç† =================
print("2. æ­£åœ¨å¤„ç†è¾“å…¥æ•°æ®...")
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages) # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å¤„ç†vision infoï¼Œè§†transformerç‰ˆæœ¬è€Œå®š
# ç®€åŒ–ç‰ˆè°ƒç”¨ï¼ˆQwen3 VL Embedding çš„ç”¨æ³•å¯èƒ½ç•¥æœ‰ä¸åŒï¼Œä»¥ä¸‹æ˜¯é€šç”¨ VL é€»è¾‘ï¼‰
# ç”±äº Qwen3-VL-Embedding æ¯”è¾ƒæ–°ï¼Œé€šå¸¸ç”¨æ³•å¦‚ä¸‹ï¼š
inputs = processor(
    text=[text],
    images=[image],
    padding=True,
    return_tensors="pt"
).to(DEVICE)

# ================= 4. ç”Ÿæˆå‘é‡ (Embedding) =================
print(f"3. æ­£åœ¨ç”Ÿæˆ {OUTPUT_DIM} ç»´å‘é‡...")

# å¼€å¯æ¨ç†æ¨¡å¼ï¼Œä¸è®¡ç®—æ¢¯åº¦ï¼ˆçœæ˜¾å­˜ï¼‰
with torch.no_grad():
    # è·å– hidden_states
    # æ³¨æ„ï¼šQwen3-VL-Embedding é€šå¸¸å–æœ€åä¸€å±‚çš„ last_token æˆ–è€…ç‰¹å®šçš„ pooling
    # è¿™é‡Œå‡è®¾å®ƒéµå¾ªæ ‡å‡† HuggingFace æ¥å£ï¼Œæˆ–è€…æˆ‘ä»¬éœ€è¦è°ƒç”¨ä¸“é—¨çš„ embedding æ–¹æ³•
    # æ ¹æ®æ–‡æ¡£ï¼Œå®ƒæ”¯æŒ dimension å‚æ•°æˆªæ–­
    
    # âš ï¸ Qwen3-Embedding çš„ç‰¹æ®Šè°ƒç”¨æ–¹å¼ï¼ˆæ¨¡æ‹Ÿï¼‰ï¼š
    # é€šå¸¸ Embedding æ¨¡å‹ä¼šè¾“å‡º last_hidden_state
    outputs = model(**inputs, output_hidden_states=True)
    
    # å–æœ€åä¸€å±‚ hidden state
    last_hidden_state = outputs.hidden_states[-1] 
    
    # å–æœ€åä¸€ä¸ª token çš„å‘é‡ä½œä¸ºæ•´ä¸ªå¥å­çš„è¡¨ç¤º (EOS token pooling)
    # æˆ–è€…å– mean poolingï¼ŒQwen å®˜æ–¹é€šå¸¸æ¨è EOS
    embeddings = last_hidden_state[:, -1, :] 

    # ğŸ”¥ å…³é”®æ­¥éª¤ï¼šç»´åº¦æˆªæ–­/æŠ•å½±
    # Qwen3-VL-Embedding æ”¯æŒ Matryoshka æˆªæ–­ï¼Œç›´æ¥åˆ‡ç‰‡å³å¯ï¼
    embeddings = embeddings[:, :OUTPUT_DIM]
    
    # å½’ä¸€åŒ– (ä½¿å¾—å‘é‡é•¿åº¦ä¸º1ï¼Œæ–¹ä¾¿è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦)
    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

# ================= 5. è¾“å‡ºç»“æœ =================
print("-" * 30)
print(f"âœ… ç”ŸæˆæˆåŠŸï¼")
print(f"å‘é‡å½¢çŠ¶: {embeddings.shape}")  # åº”è¯¥æ˜¯ [1, 768]
print(f"å‰ 10 ä½æ•°æ®: {embeddings[0, :10].cpu().numpy()}")
print("-" * 30)

# æ¸…ç†æ˜¾å­˜
del model, inputs, outputs
torch.cuda.empty_cache()