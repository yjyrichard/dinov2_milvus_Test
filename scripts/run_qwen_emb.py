import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
import requests
from io import BytesIO

# ================= é…ç½®å‚æ•° =================
MODEL_PATH = "Qwen/Qwen3-VL-Embedding-2B"
OUTPUT_DIM = 768 # ä½ æƒ³è¦çš„å‘é‡ç»´åº¦

# âš ï¸ å¿…é¡»è®¾ç½® device_map="auto"ï¼Œè®©å®ƒè‡ªåŠ¨åˆ©ç”¨å†…å­˜(RAM)
# å› ä¸º 3GB æ˜¾å­˜æ”¾ä¸ä¸‹å®Œæ•´çš„ FP16 æ¨¡å‹
print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–...")

# ================= 1. åŠ è½½æ¨¡å‹ (æŒ‰å®˜ç½‘æˆªå›¾æ–¹å¼) =================
print("1. æ­£åœ¨åŠ è½½æ¨¡å‹ (è‡ªåŠ¨åˆ†é…æ˜¾å­˜/å†…å­˜)...")

try:
    # ğŸŒŸ è¿™é‡Œæ”¹æˆäº†å®˜ç½‘æ¨èçš„ AutoModelForVision2Seq
    model = AutoModelForVision2Seq.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.float16, # ä¿æŒ FP16 ç²¾åº¦
        device_map="auto",         # æ ¸å¿ƒï¼šè§£å†³ 3GB æ˜¾å­˜ä¸è¶³çš„é—®é¢˜
    )
    
    processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    # å¦‚æœè¿™é‡ŒæŠ¥é”™ import errorï¼Œå¯èƒ½éœ€è¦å‡çº§ transformers: pip install --upgrade transformers
    exit()

# ================= 2. å‡†å¤‡æ•°æ® =================
img_url = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
try:
    image = Image.open(BytesIO(requests.get(img_url).content))
except:
    image = Image.new('RGB', (224, 224), color='white')

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": "Describe this image"}, # å¯¹äºEmbeddingæ¨¡å‹ï¼Œè¿™é‡Œå¯ä»¥æ˜¯å…·ä½“çš„æŸ¥è¯¢æˆ–æè¿°
        ],
    }
]

# ================= 3. æ¨ç†ç”Ÿæˆå‘é‡ =================
print("2. æ­£åœ¨è®¡ç®—å‘é‡...")

# é¢„å¤„ç†
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(
    text=[text],
    images=[image],
    padding=True,
    return_tensors="pt"
).to(model.device) # è‡ªåŠ¨è·Ÿéšæ¨¡å‹è®¾å¤‡

with torch.no_grad():
    # è¿è¡Œæ¨¡å‹
    # AutoModelForVision2Seq è¾“å‡ºçš„æ˜¯ BaseModelOutputWithPooling
    outputs = model(**inputs)
    
    # è·å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ (Batch_Size, Sequence_Length, Hidden_Size)
    last_hidden_state = outputs.last_hidden_state
    
    # æå–å‘é‡ç­–ç•¥ï¼šé€šå¸¸å–æœ€åä¸€ä¸ª Token (EOS) çš„ç‰¹å¾
    embeddings = last_hidden_state[:, -1, :] 

    # ğŸ¤ ç»´åº¦æˆªæ–­ (Matryoshka Embedding)
    # Qwen3-VL-Embedding æ”¯æŒå¼¹æ€§ç»´åº¦ï¼Œå¦‚æœä½ åªéœ€è¦ 768 ç»´ï¼Œç›´æ¥åˆ‡ç‰‡å³å¯
    embeddings = embeddings[:, :OUTPUT_DIM]
    
    # å½’ä¸€åŒ– (è¿™ä¸€æ­¥å¯¹ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢è‡³å…³é‡è¦)
    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

# ================= 4. æ‰“å°ç»“æœ =================
print("-" * 30)
print(f"âœ… ç”ŸæˆæˆåŠŸï¼")
print(f"å‘é‡ç»´åº¦: {embeddings.shape}") # åº”è¯¥æ˜¯ [1, 768]
print(f"å‰ 10 ä½æ•°å€¼: {embeddings[0, :10].cpu().numpy()}")
print("-" * 30)

# æ˜¾å­˜æ¸…ç†å»ºè®®
# å¦‚æœåé¢è¿˜è¦è·‘å…¶ä»–ä¸œè¥¿ï¼Œå»ºè®®æŠŠ model del æ‰
del model
torch.cuda.empty_cache()